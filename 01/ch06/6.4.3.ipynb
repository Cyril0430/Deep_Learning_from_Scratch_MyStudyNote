{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a875c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "import numpy as np\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio   # np.random.rand()には整数しか入らないが、引き数に「*(N, D)」とすれば、tuple形式での指定も可能になる。この場合、各要素0から1の間の値である(N, D)の形状のNumPy配列が生成される。\n",
    "            # 正確に言えば、このアスタリスク「*」は、タプルを展開（unpacking）し、`rand`関数が求める「複数の引数」として渡しているというPythonのテクニックである。\n",
    "            \n",
    "            return x * self.mask        # False の要素は「0」になる。\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)   # ここでは何をしている？\n",
    "            # →訓練時は全体の「self.dropout_ratio」の割合で信号を送らないニューロンを指定した。つまり信号を送ったニューロンの割合は「1 - self.dropout_ratio」である。そのためテスト時も、訓練時の信号の強さと等しくする（そうしないと訓練時の状況に遭わせていることにならない。）ために、入力信号全体に「(1 - self.dropout_ratio)」を掛ける。そうすることで、訓練時とテスト時で整合性（外的妥当性）が保証される。\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask     # 順伝播の時に通したところだけが伝播していく。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
